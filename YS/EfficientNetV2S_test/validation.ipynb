{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, ConcatDataset, WeightedRandomSampler\n",
    "\n",
    "from dataset import MaskBaseDataset\n",
    "from loss import create_criterion\n",
    "import copy\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mo2rabbit\u001b[0m (\u001b[33mcv13\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/ml/workspace/YS/EfficientNetV2S_test/wandb/run-20221107_043155-37cj8erq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cv13/EfficientNetV2s_YS/runs/37cj8erq\" target=\"_blank\">sleek-resonance-11</a></strong> to <a href=\"https://wandb.ai/cv13/EfficientNetV2s_YS\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login() # Í∞ÅÏûê WandB Î°úÍ∑∏Ïù∏ ÌïòÍ∏∞\n",
    "# 9eee70600a60d9d41eecef494a78a696bd12d252\n",
    "\n",
    "# üêù initialise a wandb run\n",
    "wandb.init(\n",
    "    project=\"EfficientNetV2s_YS\", # ÌîÑÎ°úÏ†ùÌä∏ Ïù¥Î¶Ñ \"Î™®Îç∏_Î≤ÑÏ†Ñ_ÏÑ±Î™Ö\"\n",
    "    config = {\n",
    "    \"lr\": 0.0001,\n",
    "    \"epochs\": 60,\n",
    "    \"batch_size\": 64,\n",
    "    \"optimizer\" : \"Adam\",\n",
    "    \"resize\" : [384, 384],\n",
    "    \"criterion\" : 'weight_cross_entropy'\n",
    "    }\n",
    " )\n",
    "\n",
    "# Copy your config \n",
    "config = wandb.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--seed SEED] [--epochs EPOCHS]\n",
      "                             [--dataset DATASET]\n",
      "                             [--preprocessing PREPROCESSING]\n",
      "                             [--RealAugmentation REALAUGMENTATION]\n",
      "                             [--resize RESIZE [RESIZE ...]]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--valid_batch_size VALID_BATCH_SIZE]\n",
      "                             [--model MODEL] [--optimizer OPTIMIZER] [--lr LR]\n",
      "                             [--val_ratio VAL_RATIO] [--criterion CRITERION]\n",
      "                             [--lr_decay_step LR_DECAY_STEP]\n",
      "                             [--log_interval LOG_INTERVAL] [--name NAME]\n",
      "                             [--data_dir DATA_DIR] [--model_dir MODEL_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"b3cb22eb-d1a1-4df9-84df-ad4e7254548a\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=/opt/ml/.local/share/jupyter/runtime/kernel-v2-27184KCVabrT59v1o.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Data and model checkpoints directories\n",
    "parser.add_argument('--seed', type=int, default=42, help='random seed (default: 42)')\n",
    "parser.add_argument('--epochs', type=int, default=config.epochs, help='number of epochs to train (default: 1)')\n",
    "parser.add_argument('--dataset', type=str, default='MaskBaseDataset', help='dataset augmentation type (default: MaskBaseDataset)')\n",
    "parser.add_argument('--preprocessing', type=str, default='Basepreprocessing', help='data augmentation type (default: Basepreprocessing)')\n",
    "parser.add_argument('--RealAugmentation', type=str, default='RealAugmentation', help='data augmentation type (default: RealAugmentation)')\n",
    "parser.add_argument(\"--resize\", nargs=\"+\", type=list, default=config.resize, help='resize size for image when training')\n",
    "parser.add_argument('--batch_size', type=int, default=config.batch_size, help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--valid_batch_size', type=int, default=250, help='input batch size for validing (default: 1000)')\n",
    "parser.add_argument('--model', type=str, default='efficientnet_v2_s', help='model type (default: BaseModel)')\n",
    "parser.add_argument('--optimizer', type=str, default=config.optimizer, help='optimizer type (default: SGD)')\n",
    "parser.add_argument('--lr', type=float, default=config.lr, help='learning rate (default: 1e-3)')\n",
    "parser.add_argument('--val_ratio', type=float, default=0.2, help='ratio for validaton (default: 0.2)')\n",
    "parser.add_argument('--criterion', type=str, default=config.criterion, help='criterion type (default: cross_entropy)')\n",
    "parser.add_argument('--lr_decay_step', type=int, default=10, help='learning rate scheduler deacy step (default: 20)')\n",
    "parser.add_argument('--log_interval', type=int, default=20, help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--name', default='exp', help='model save at {SM_MODEL_DIR}/{name}')\n",
    "\n",
    "# Container environment\n",
    "parser.add_argument('--data_dir', type=str, default=os.environ.get('SM_CHANNEL_TRAIN', '/opt/ml/input/data/train/images'))\n",
    "parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR', './model'))\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "# data_dir = args.data_dir\n",
    "# model_dir = args.model_dir\n",
    "\n",
    "# train(data_dir, model_dir, args)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "\n",
    "save_dir = increment_path(os.path.join(model_dir, args.name))\n",
    "\n",
    "# -- settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# -- dataset\n",
    "dataset_module = getattr(import_module(\"dataset\"), args.dataset)  # default: MaskBaseDataset\n",
    "dataset = dataset_module(\n",
    "    data_dir=data_dir,\n",
    ")\n",
    "num_classes = dataset.num_classes  # 18\n",
    "\n",
    "\n",
    "\n",
    "# -- preprocessing --data_set\n",
    "transform_module = getattr(import_module(\"dataset\"), args.preprocessing)  # default: preprocessing\n",
    "transform = transform_module(\n",
    "    resize=args.resize,\n",
    "    mean=dataset.mean,\n",
    "    std=dataset.std,\n",
    ")\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "dataset_aug = copy.deepcopy(dataset)\n",
    "\n",
    "# augmentation Ï†ÅÏö©\n",
    "transform_module_aug = getattr(import_module(\"dataset\"), args.RealAugmentation)  # default: RealAugmentation\n",
    "transform_aug = transform_module_aug(\n",
    "    resize=args.resize,\n",
    "    mean=dataset_aug.mean,\n",
    "    std=dataset_aug.std,\n",
    ")\n",
    "dataset_aug.set_transform(transform_aug)\n",
    "\n",
    "\n",
    "train_set,val_set = dataset.split_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
